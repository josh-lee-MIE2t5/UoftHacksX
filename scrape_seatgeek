import requests
from datetime import datetime
from bs4 import BeautifulSoup
import json

# Description: This program scrapes three webpages from seatgeek to find event information.
# This scraper finds the event name, the time and place of the event, whether registration is needed, and its frequency.
# Currently, only scrapes 10 visible items from this particular website.
# Need to figure out if this is the best method. Also see if playwright can be used alongside to automate and allow scraping for more content.

# baseurl and other webpages we want to scrape
baseurl = 'https://seatgeek.com/'
webpages = ['https://seatgeek.com/cities/toronto/concerts', 'https://seatgeek.com/cities/toronto/sports', 'https://seatgeek.com/cities/toronto/theater']

# headers just apparently lowers the likelihood of us getting IP banned for doing this shit lmao
headers = {
    'User-Agent': 'https://developers.whatismybrowser.com/useragents/parse/?analyse-my-user-agent=yes'
}

json_events = []

# outer for loop to loop through all webpages
for i in range(0,3):
    
    r = requests.get(webpages[i]) # accesses webpage
    soup = BeautifulSoup(r.content, 'lxml') # this gets all content from page, 'lxml' is the parser

    eventLinks = []
    
    # finds all elements on the webpage matching this type, which in this case is the concert rectangle on the page.
    # note: this may need to be updated regularly, since I had to change this from 6 months ago to get it to work.
    tabContent = soup.find_all('li', class_="EventItem__Item-sc-d3985126-5 iIzFSE EventItem-sc-d3985126-17 dqTfWI")

    # For each element, find all links to them and store into eventLinks. 
    for item in tabContent:
        for link in item.find_all('a', href=True):
            eventLinks.append(baseurl + link['href'])
    print("Links Loaded! Starting Search...")

    # Loops through each link (i.e. each event)
    for link in eventLinks:
        r = requests.get(link, headers = headers)
        soup = BeautifulSoup(r.content, 'lxml')

        # Tries to find title of each event, same method as before.
        try:
            title = soup.find('h1', class_="DesktopListingsHeader__EventTitle-sc-353e9155-1 hYLbGU").text.strip()
        except:
            title = "title unavailable at the moment"
        description = None
        if i == 0:
            description = "concert"
        elif i == 1:
            description = "sports event"
        else:
            description = "theatre event"
        
        endDate = "that day"
        frequency = "once"
        regRequirement = "true"

        # Tries to find time and place for each event, same method as before.
        try:
            time_place = soup.find('p', class_="Typography__Text3-sc-bf16a43d-10 hkindF").text.strip().split("Â·")
        except:
            startDate = "start time unavailable at the moment"
        at_index = time_place[0].rfind("at")

        #takes date/time from website -> converts to proper form
        startDate = (datetime.strptime((time_place[0][0:at_index] + time_place[0][at_index+2::]).strip(), "%a %b %d  %I:%M%p"))
        startDate = startDate.replace(year=2023)

        #stores all data as a JSON object to send to web application.
        event = {
            'title': title,
            'description': description,
            'location': time_place[1],
            'startDate': startDate.strftime("%Y:%m:%d %H:%M:%S"),
            'endDate': endDate,
            "registrationReq": regRequirement,
            "frequency": frequency
        }

        json_events.append(event)
        print("Saving: ", event['title'])

#prints the entire json object array
print(json.dumps(json_events))

# format of JSON object string
# "title": "event 2", DONE
#    "description": "death", NONE
#    "location": "my hall", DONE
#    "_type": "competition", DONE
#    "startDate": "2023-01-21", DONE
#    "endDate": "2023-01-25", NONE
#    "registrationReq": false, DONE
#    "frequency": "yearly"''' + toParse, NONE